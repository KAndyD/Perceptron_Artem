
# Лабораторная работа: Реализация и обучение перцептрона «Артем»

## Цель

Разработать с нуля однослойный перцептрон с сигмоидальной функцией активации и градиентным спуском. Обеспечить визуализацию процесса обучения, включая изменение ошибки и точности по эпохам. Проверить работу модели на синтетических данных.

---

## Используемые библиотеки

| Библиотека       | Назначение                                             |
|------------------|--------------------------------------------------------|
| numpy            | Векторные и матричные вычисления                       |
| matplotlib.pyplot| Визуализация ошибок и точности                         |
| sklearn.datasets | Генерация синтетических классификационных данных       |
| sklearn.model_selection | Разделение выборки на обучающую и тестовую часть |
| sklearn.preprocessing | Нормализация данных (стандартизация признаков)    |

---

## Алгоритм: Перцептрон с сигмоидой (модель «Артем»)

Модель реализована вручную. Отличительные особенности:

- Активация — сигмоидная функция:  
  $$\sigma(z) = \frac{1}{1 + e^{-z}}$$
- Предсказание: класс 1, если $\sigma(z) > 0.5$, иначе 0
- Функция потерь: среднеквадратичная ошибка (MSE)
 ![изображение](https://github.com/user-attachments/assets/66ac5742-d5b4-449a-9840-002077397ba6)

- Обновление весов: градиентный спуск  
  ![изображение](https://github.com/user-attachments/assets/6f03f64d-68f8-4745-8fd3-0a1920ac6c97)


### Основные методы:

#### `__init__(self, input_size, learning_rate=0.1, epochs=1000)`

- Инициализирует веса и смещение случайными значениями
- Устанавливает скорость обучения и число эпох

#### `sigmoid(self, z)`

- Вычисляет сигмоиду от входа z

#### `sigmoid_derivative(self, z)`

- Вычисляет производную сигмоиды для градиентного спуска

#### `predict(self, x)`

- Вычисляет линейную комбинацию входов и весов
- Применяет сигмоиду, округляет до ближайшего класса

#### `train(self, X, d)`

- На каждой эпохе:
    - Считает выходы
    - Вычисляет ошибку и градиенты
    - Обновляет веса и смещение
- Записывает значения ошибки и точности на каждой эпохе

---

## Этапы работы

### 1. Генерация данных

Изначально для быстрой отладки и тестирования модели был создан небольшой синтетический датасет с помощью функции `make_classification`:

```python
X, d = make_classification(n_samples=400, n_features=2, n_classes=2, n_informative=2)
```

- **n_samples=400** — 400 обучающих примеров, что позволяет быстро проверить корректность реализации.
- **n_features=2** — всего два признака, что удобно для последующей визуализации.
- **n_informative=2** — оба признака полезны для разделения классов.
- **n_classes=2** — задача бинарной классификации.

После успешной отладки модель была масштабирована на **1 000 000 примеров**, что позволило:
- протестировать её **устойчивость и производительность** при больших объёмах данных,
- оценить **масштабируемость градиентного спуска**,
- и подтвердить, что модель может обучаться эффективно даже при высокой нагрузке.

---

### 2. Нормализация данных

Для ускорения сходимости и стабильности процесса обучения применяется стандартизация признаков с помощью `StandardScaler`:

```python
scaler = StandardScaler()
X = scaler.fit_transform(X)
```

- Приводит все признаки к **нулевому среднему** и **единичному стандартному отклонению**.
- Это важно для градиентного спуска, поскольку без нормализации веса могут обновляться неравномерно из-за разного масштаба признаков.

---

### 3. Обучение модели

Модель инициализируется и обучается на обучающей выборке:

```python
model = Artem(input_size=2, learning_rate=0.1, epochs=1000)
model.train(X_train, y_train)
```

- **input_size = 2** — количество признаков.
- **learning_rate = 0.1** — скорость обновления весов.
- **epochs = 1000** — количество итераций обучения.

Во время обучения:
- Модель вычисляет сигмоидный выход для каждого входа.
- Считает ошибку и производит **обновление весов** и **смещения** на основе градиентного спуска.
- Ошибка и точность фиксируются на каждой эпохе, что даёт возможность построить информативные графики.

---

### 4. Проверка качества

После обучения модель тестируется на отложенной тестовой выборке. Оценивается точность:

```python
accuracy = model.evaluate(X_test, y_test)
print(f"Точность: {accuracy:.2f}")
```

- Предсказания округляются: 1, если $\sigma(z) > 0.5$, иначе — 0.
- Вычисляется доля верных ответов.
- Полученная точность (обычно **более 85%**) говорит о высокой обучаемости модели.

---

## Визуализация

### Ошибка по эпохам

График среднеквадратичной ошибки (MSE) на обучающей выборке:

![изображение](https://github.com/user-attachments/assets/0cb76d95-add4-46a1-b860-ac08f6f492b5)


### Точность по эпохам

Как модель улучшает свои предсказания во времени:

![изображение](https://github.com/user-attachments/assets/2739802f-71fa-4077-97f3-4857463fc60a)


---

## Результаты

- Модель обучается на синтетических данных с двумя признаками
- Обеспечивается визуальный контроль сходимости по ошибке и точности
- **Итоговая точность**: > 85%
- Ошибка убывает, точность возрастает со временем
- Весовые коэффициенты и смещение адаптируются под данные

---
## Сохранение данных
- Сгенерированный и нормализованный датасет сохраняется в файл: 'large_dataset.csv'
---

## Заключение

Перцептрон «Артем» успешно реализован с нуля. Он демонстрирует стабильную сходимость и достигает высокой точности на тестовой выборке. Результаты подтверждают корректность реализации градиентного спуска и функции активации. Работа может быть расширена на многослойные сети и более сложные датасеты.

---
